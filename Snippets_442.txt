{
  "Differentiation Methods": {
    "prefix": "differentiation",
    "body": [
      "import numpy as np",
      "import matplotlib.pyplot as plt",
      "",
      "# Forward difference (n+1 formula)",
      "def forward_difference(f, x, h):",
      "    \"\"\"",
      "    Approximate the derivative of f at x using the forward difference formula.",
      "    f: function to differentiate",
      "    x: point at which to evaluate the derivative",
      "    h: step size",
      "    \"\"\"",
      "    return (f(x + h) - f(x)) / h",
      "",
      "# Three-point central difference formula",
      "def three_point_formula(f, x, h):",
      "    \"\"\"",
      "    Approximate the derivative of f at x using the three-point formula.",
      "    f: function to differentiate",
      "    x: point at which to evaluate the derivative",
      "    h: step size",
      "    \"\"\"",
      "    return (f(x + h) - f(x - h)) / (2 * h)",
      "",
      "# True derivative of f(x) = e^x * sin(x)",
      "def true_derivative(x):",
      "    return np.exp(x) * np.sin(x) + np.exp(x) * np.cos(x)",
      "",
      "# Define a more complex function to differentiate: f(x) = e^x * sin(x)",
      "def f(x):",
      "    return np.exp(x) * np.sin(x)",
      "",
      "# Range of x values for plotting",
      "x_values = np.linspace(0.1, 5, 100)",
      "h = 0.01  # Step size for numerical differentiation",
      "",
      "# Compute the true derivatives",
      "true_values = true_derivative(x_values)",
      "",
      "# Compute the forward difference and three-point formula results",
      "forward_values = forward_difference(f, x_values, h)",
      "three_point_values = three_point_formula(f, x_values, h)",
      "",
      "# Compute the errors",
      "forward_error = np.abs(true_values - forward_values)",
      "three_point_error = np.abs(true_values - three_point_values)",
      "",
      "# Plot the numerical derivatives and the true derivative",
      "plt.figure(figsize=(12, 6))",
      "",
      "# Plot True Derivative",
      "plt.plot(x_values, true_values, label=\"True Derivative (f'(x) = 2x)\", color='black', linestyle='--')",
      "",
      "# Plot Forward Difference (n+1 formula)",
      "plt.plot(x_values, forward_values, label=\"Forward Difference (n+1)\", color='blue')",
      "",
      "# Plot Three-Point Formula",
      "plt.plot(x_values, three_point_values, label=\"Three-Point Formula\", color='red')",
      "",
      "# Customize the plot",
      "plt.title('Numerical Differentiation: Forward Difference vs Three-Point Formula')",
      "plt.xlabel('x')",
      "plt.ylabel('Derivative')",
      "plt.legend()",
      "plt.grid(True)",
      "plt.show()",
      "",
      "# Plot the errors",
      "plt.figure(figsize=(12, 6))",
      "",
      "# Plot Error for Forward Difference",
      "plt.plot(x_values, forward_error, label=\"Error in Forward Difference\", color='blue', linestyle='--')",
      "",
      "# Plot Error for Three-Point Formula",
      "plt.plot(x_values, three_point_error, label=\"Error in Three-Point Formula\", color='red', linestyle='--')",
      "",
      "# Customize the error plot",
      "plt.title('Error in Numerical Differentiation Methods')",
      "plt.xlabel('x')",
      "plt.ylabel('Error')",
      "plt.legend()",
      "plt.grid(True)",
      "plt.show()"
    ]
  },
    "Integration Methods": {
      "prefix": "integration",
      "body": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "# Trapezoidal Rule Implementation",
        "def trapezoidal_rule(f, a, b, n):",
        "    \"\"\"",
        "    Approximate the integral of f(x) from a to b using the Trapezoidal Rule.",
        "    f: function to integrate",
        "    a: lower bound of integration",
        "    b: upper bound of integration",
        "    n: number of subintervals",
        "    \"\"\"",
        "    # Step size",
        "    h = (b - a) / n",
        "    ",
        "    # Compute the sum using the trapezoidal rule",
        "    integral = 0.5 * (f(a) + f(b))  # First and last terms",
        "    for i in range(1, n):",
        "        integral += f(a + i * h)  # Middle terms",
        "    ",
        "    # Multiply by the step size",
        "    integral *= h",
        "    ",
        "    return integral",
        "",
        "# Define a function to integrate, for example f(x) = x^2",
        "def f(x):",
        "    return x**2",
        "",
        "# Set the bounds and the number of subintervals",
        "a = 0  # Lower bound",
        "b = 1  # Upper bound",
        "n = 100  # Number of subintervals",
        "",
        "# Compute the integral using the Trapezoidal Rule",
        "result = trapezoidal_rule(f, a, b, n)",
        "print(f\"Approximate integral of f(x) = x^2 from {a} to {b} is: {result}\")",
        "",
        "# Compare with the exact integral",
        "exact_integral = (b**3 - a**3) / 3  # Exact integral of x^2 from a to b",
        "print(f\"Exact integral is: {exact_integral}\")",
        "",
        "",
        "# Plot the function and the trapezoidal approximation",
        "x_vals = np.linspace(a, b, 1000)",
        "y_vals = f(x_vals)",
        "",
        "# Plot the function",
        "plt.plot(x_vals, y_vals, label=\"f(x) = x^2\", color='blue')",
        "",
        "# Plot the trapezoids",
        "x_trap = np.linspace(a, b, n+1)",
        "y_trap = f(x_trap)",
        "for i in range(n):",
        "    plt.fill_between(x_trap[i:i+2], y_trap[i:i+2], alpha=0.3, color='orange')",
        "",
        "# Add labels and legend",
        "plt.title('Trapezoidal Rule Approximation for f(x) = x^2')",
        "plt.xlabel('x')",
        "plt.ylabel('f(x)')",
        "plt.legend()",
        "plt.grid(True)",
        "plt.show()",
        "",
        "# Calculate error",
        "error = abs(result - exact_integral)",
        "print(f\"Error in Trapezoidal Rule Approximation: {error}\")"
      ]
    },
      "Euler's Method": {
        "prefix": "euler",
        "body": [
          "import numpy as np",
          "",
          "def euler_method(f, x0, y0, x_end, h):",
          "    x_values = [x0]",
          "    y_values = [y0]",
          "    ",
          "    x = x0",
          "    y = y0",
          "    ",
          "    while x < x_end:",
          "        y = y + h * f(x, y)",
          "        x = x + h",
          "        ",
          "        x_values.append(x)",
          "        y_values.append(y)",
          "    ",
          "    return np.array(x_values), np.array(y_values)",
          "",
          "import numpy as np",
          "import matplotlib.pyplot as plt",
          "",
          "def example_function(x, y):",
          "    return x - y  # Example ODE: dy/dx = x - y",
          "",
          "def exact_solution(x):",
          "    return x - 1 + 2 * np.exp(-x)",
          "",
          "x0 = 0",
          "y0 = 1",
          "x_end = 2",
          "h = 0.1",
          "",
          "x_vals, y_vals = euler_method(example_function, x0, y0, x_end, h)",
          "",
          "exact_y_vals = exact_solution(x_vals)",
          "",
          "# Print table header",
          "print(\"x         Euler y        Exact y        Error\")",
          "print(\"-\" * 45)",
          "",
          "# Print values in tabular format",
          "for x, y_euler, y_exact in zip(x_vals, y_vals, exact_y_vals):",
          "    error = abs(y_exact - y_euler)",
          "    print(f\"{x:.2f}      {y_euler:.7f}      {y_exact:.7f}      {error:.7f}\")",
          "",
          "# Plot the solution",
          "plt.plot(x_vals, y_vals, label=\"Euler's Method Solution\", marker='o')",
          "plt.plot(x_vals, exact_y_vals, label=\"Exact Solution\", linestyle='--')",
          "plt.xlabel(\"x\")",
          "plt.ylabel(\"y\")",
          "plt.legend()",
          "plt.grid()",
          "plt.show()"
        ]
      },
        "Runge-Kutta 4th Order": {
          "prefix": "runge-kutta",
          "body": [
            "import numpy as np",
            "",
            "def runge_kutta_4th_order(f, x0, y0, x_end, h):",
            "    x_values = [x0]",
            "    y_values = [y0]",
            "    ",
            "    x = x0",
            "    y = y0",
            "    ",
            "    while x < x_end:",
            "        k1 = f(x, y)",
            "        k2 = f(x + h / 2, y + h * k1 / 2)",
            "        k3 = f(x + h / 2, y + h * k2 / 2)",
            "        k4 = f(x + h, y + h * k3)",
            "        ",
            "        y = y + (h / 6) * (k1 + 2 * k2 + 2 * k3 + k4)",
            "        x = x + h",
            "        ",
            "        x_values.append(x)",
            "        y_values.append(y)",
            "    ",
            "    return np.array(x_values), np.array(y_values)",
            "",
            "import numpy as np",
            "import matplotlib.pyplot as plt",
            "",
            "# Differential equation: dy/dx = x - y",
            "def example_function(x, y):",
            "    return x - y",
            "",
            "# Exact solution of the differential equation",
            "def exact_solution(x):",
            "    return x - 1 + 2 * np.exp(-x)",
            "",
            "# Parameters",
            "x0 = 0",
            "y0 = 1",
            "x_end = 2",
            "h = 0.1",
            "",
            "# Solve using Runge-Kutta",
            "x_vals, y_vals = runge_kutta_4th_order(example_function, x0, y0, x_end, h)",
            "",
            "# Compute exact values",
            "exact_y_vals = exact_solution(x_vals)",
            "",
            "# Print table header",
            "print(\"x         RK4 y          Exact y        Error\")",
            "print(\"-\" * 45)",
            "",
            "# Print values in tabular format",
            "for x, y_rk4, y_exact in zip(x_vals, y_vals, exact_y_vals):",
            "    error = abs(y_exact - y_rk4)",
            "    print(f\"{x:.2f}      {y_rk4:.7f}      {y_exact:.7f}      {error:.7f}\")",
            "",
            "# Plot the solution",
            "plt.plot(x_vals, y_vals, label=\"Runge-Kutta 4th Order\", marker='o')",
            "plt.plot(x_vals, exact_y_vals, label=\"Exact Solution\", linestyle='--')",
            "plt.xlabel(\"x\")",
            "plt.ylabel(\"y\")",
            "plt.legend()",
            "plt.grid()",
            "plt.title(\"Runge-Kutta 4th Order vs Exact Solution\")",
            "plt.show()"
          ]
        },
          "Matrix Inversion": {
            "prefix": "matrix-inversion",
            "body": [
              "import numpy as np",
              "",
              "# Define the matrix A and vector b",
              "A = np.array([[3, 1], [1, 2]])",
              "b = np.array([9, 8])",
              "",
              "# Compute the inverse of A",
              "A_inv = np.linalg.inv(A)",
              "",
              "# Compute the solution x",
              "x = np.dot(A_inv, b)",
              "",
              "print(\"Solution x:\", x)"
            ]
          },
            "Gaussian Elimination": {
              "prefix": "gauss-elim",
              "body": [
                "import numpy as np",
                "",
                "def gaussian_elimination(A, b):",
                "    n = len(b)",
                "    # Augmented matrix [A|b]",
                "    Ab = np.hstack([A, b.reshape(-1, 1)])",
                "    ",
                "    # Forward elimination",
                "    for i in range(n):",
                "        # Make the diagonal element 1 (pivoting)",
                "        pivot = Ab[i, i]",
                "        if pivot == 0:",
                "            raise ValueError(\"Matrix is singular, cannot proceed with Gaussian elimination.\")",
                "        ",
                "        # Scale the row",
                "        Ab[i] = Ab[i] / pivot",
                "        ",
                "        # Eliminate the elements below the pivot",
                "        for j in range(i + 1, n):",
                "            scale = Ab[j, i]",
                "            Ab[j] = Ab[j] - scale * Ab[i]",
                "    ",
                "    # Back substitution",
                "    x = np.zeros(n)",
                "    for i in range(n - 1, -1, -1):",
                "        x[i] = Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:])",
                "    ",
                "    return x",
                "",
                "# Define the matrix A and vector b",
                "A = np.array([[3, 1], [1, 2]], dtype=float)",
                "b = np.array([9, 8], dtype=float)",
                "",
                "# Solve the system using Gaussian Elimination",
                "x = gaussian_elimination(A, b)",
                "print(\"Solution x:\", x)"
              ]
            },
              "Gaussian Elimination No Pivot Back Substitution": {
                "prefix": "gauss-elim-no-pivot-backsub",
                "body": [
                  "import numpy as np",
                  "",
                  "def gaussian_elimination_no_pivot(A, b):",
                  "    n = len(b)",
                  "    # Augmented matrix [A|b]",
                  "    Ab = np.hstack([A, b.reshape(-1, 1)])",
                  "",
                  "    # Forward Elimination",
                  "    for i in range(n):",
                  "        for j in range(i + 1, n):",
                  "            if Ab[j, i] != 0:  # Avoid division by zero",
                  "                factor = Ab[j, i] / Ab[i, i]",
                  "                Ab[j] = Ab[j] - factor * Ab[i]",
                  "",
                  "    # Back Substitution",
                  "    x = np.zeros(n)",
                  "    for i in range(n - 1, -1, -1):",
                  "        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:])) / Ab[i, i]",
                  "",
                  "    return x",
                  "",
                  "# Example system of equations",
                  "A = np.array([[3, 1, 2], [1, 2, 3], [2, 1, 3]], dtype=float)",
                  "b = np.array([9, 8, 7], dtype=float)",
                  "",
                  "# Solve the system using Gaussian elimination (no pivoting)",
                  "x = gaussian_elimination_no_pivot(A, b)",
                  "print(\"Solution x:\", x)"
                ]
              },
                "LU Decomposition and Solve System": {
                  "prefix": "lu-factor",
                  "body": [
                    "import numpy as np",
                    "",
                    "def lu_decomposition(A):",
                    "    \"\"\"",
                    "    Perform LU Decomposition on matrix A.",
                    "    Returns the lower triangular matrix L and upper triangular matrix U.",
                    "    \"\"\"",
                    "    n = A.shape[0]",
                    "    L = np.zeros_like(A)",
                    "    U = np.zeros_like(A)",
                    "",
                    "    for i in range(n):",
                    "        # Upper triangular matrix",
                    "        for j in range(i, n):",
                    "            U[i, j] = A[i, j] - np.dot(L[i, :i], U[:i, j])",
                    "        ",
                    "        # Lower triangular matrix",
                    "        for j in range(i, n):",
                    "            if i == j:",
                    "                L[i, i] = 1  # Diagonal elements of L are 1",
                    "            else:",
                    "                L[j, i] = (A[j, i] - np.dot(L[j, :i], U[:i, i])) / U[i, i]",
                    "",
                    "    return L, U",
                    "",
                    "def forward_substitution(L, b):",
                    "    \"\"\"",
                    "    Solve Ly = b using forward substitution.",
                    "    \"\"\"",
                    "    n = len(b)",
                    "    y = np.zeros_like(b, dtype=float)",
                    "    ",
                    "    for i in range(n):",
                    "        y[i] = b[i] - np.dot(L[i, :i], y[:i])",
                    "    ",
                    "    return y",
                    "",
                    "def backward_substitution(U, y):",
                    "    \"\"\"",
                    "    Solve Ux = y using backward substitution.",
                    "    \"\"\"",
                    "    n = len(y)",
                    "    x = np.zeros_like(y, dtype=float)",
                    "    ",
                    "    for i in range(n-1, -1, -1):",
                    "        x[i] = (y[i] - np.dot(U[i, i+1:], x[i+1:])) / U[i, i]",
                    "    ",
                    "    return x",
                    "",
                    "def solve_system(A, b):",
                    "    \"\"\"",
                    "    Solve the system of linear equations Ax = b using LU decomposition.",
                    "    \"\"\"",
                    "    # Step 1: Perform LU decomposition",
                    "    L, U = lu_decomposition(A)",
                    "    ",
                    "    # Step 2: Solve Ly = b",
                    "    y = forward_substitution(L, b)",
                    "    ",
                    "    # Step 3: Solve Ux = y",
                    "    x = backward_substitution(U, y)",
                    "    ",
                    "    return x",
                    "",
                    "# Example Usage",
                    "A = np.array([[4, 3], [6, 3]], dtype=float)",
                    "b = np.array([10, 12], dtype=float)",
                    "",
                    "x = solve_system(A, b)",
                    "",
                    "print(\"Solution vector x:\")",
                    "print(x)"
                  ]
                },
                  "Jacobi Iterative Method": {
                    "prefix": "jacobi",
                    "body": [
                      "import numpy as np",
                      "",
                      "def jacobi(A, b, x0=None, tol=1e-10, max_iter=1000):",
                      "    \"\"\"",
                      "    Solve Ax = b using the Jacobi iterative method without using numpy.linalg.",
                      "    ",
                      "    Parameters:",
                      "    A (ndarray): Coefficient matrix (n x n).",
                      "    b (ndarray): Right-hand side vector (n).",
                      "    x0 (ndarray): Initial guess for the solution (n). Default is None, which initializes to zeros.",
                      "    tol (float): Tolerance for convergence. Default is 1e-10.",
                      "    max_iter (int): Maximum number of iterations. Default is 1000.",
                      "    ",
                      "    Returns:",
                      "    x (ndarray): Solution vector (n).",
                      "    \"\"\"",
                      "    n = len(b)",
                      "    ",
                      "    if x0 is None:",
                      "        x0 = np.zeros(n)",
                      "    ",
                      "    x = np.copy(x0)",
                      "    for k in range(max_iter):",
                      "        x_new = np.zeros_like(x)",
                      "        ",
                      "        for i in range(n):",
                      "            # Summation of A[i, j] * x[j] for all j != i",
                      "            summation = 0",
                      "            for j in range(n):",
                      "                if i != j:",
                      "                    summation += A[i, j] * x[j]",
                      "            x_new[i] = (b[i] - summation) / A[i, i]",
                      "        ",
                      "        # Check for convergence (using infinity norm to check the change in x)",
                      "        max_diff = 0",
                      "        for i in range(n):",
                      "            max_diff = max(max_diff, abs(x_new[i] - x[i]))",
                      "        ",
                      "        if max_diff < tol:",
                      "            print(f\"Converged in {k + 1} iterations.\")",
                      "            return x_new",
                      "        ",
                      "        x = x_new",
                      "    ",
                      "    print(\"Maximum iterations reached.\")",
                      "    return x",
                      "",
                      "# Example usage:",
                      "A = np.array([[4, -1, 0, 0],",
                      "              [-1, 4, -1, 0],",
                      "              [0, -1, 4, -1],",
                      "              [0, 0, -1, 3]], dtype=float)",
                      "",
                      "b = np.array([15, 10, 10, 10], dtype=float)",
                      "",
                      "# Initial guess can be omitted, and it defaults to zero vector",
                      "solution = jacobi(A, b)",
                      "",
                      "print(\"Solution:\", solution)"
                    ]
                  },
                    "Gauss-Seidel Iterative Method": {
                      "prefix": "gauss-seidel",
                      "body": [
                        "import numpy as np",
                        "",
                        "def gauss_seidel(A, b, x0=None, tol=1e-10, max_iter=1000):",
                        "    \"\"\"",
                        "    Solve Ax = b using the Gauss-Seidel iterative method without using numpy.linalg.",
                        "    ",
                        "    Parameters:",
                        "    A (ndarray): Coefficient matrix (n x n).",
                        "    b (ndarray): Right-hand side vector (n).",
                        "    x0 (ndarray): Initial guess for the solution (n). Default is None, which initializes to zeros.",
                        "    tol (float): Tolerance for convergence. Default is 1e-10.",
                        "    max_iter (int): Maximum number of iterations. Default is 1000.",
                        "    ",
                        "    Returns:",
                        "    x (ndarray): Solution vector (n).",
                        "    \"\"\"",
                        "    n = len(b)",
                        "    ",
                        "    if x0 is None:",
                        "        x0 = np.zeros(n)",
                        "    ",
                        "    x = np.copy(x0)",
                        "    for k in range(max_iter):",
                        "        x_old = np.copy(x)",
                        "        ",
                        "        for i in range(n):",
                        "            # Summation of A[i, j] * x[j] for all j != i",
                        "            summation = 0",
                        "            for j in range(n):",
                        "                if i != j:",
                        "                    summation += A[i, j] * x[j] if j < i else A[i, j] * x_old[j]",
                        "            x[i] = (b[i] - summation) / A[i, i]",
                        "        ",
                        "        # Check for convergence (using infinity norm to check the change in x)",
                        "        max_diff = 0",
                        "        for i in range(n):",
                        "            max_diff = max(max_diff, abs(x[i] - x_old[i]))",
                        "        ",
                        "        if max_diff < tol:",
                        "            print(f\"Converged in {k + 1} iterations.\")",
                        "            return x",
                        "    ",
                        "    print(\"Maximum iterations reached.\")",
                        "    return x",
                        "",
                        "# Example usage:",
                        "A = np.array([[4, -1, 0, 0],",
                        "              [-1, 4, -1, 0],",
                        "              [0, -1, 4, -1],",
                        "              [0, 0, -1, 3]], dtype=float)",
                        "",
                        "b = np.array([15, 10, 10, 10], dtype=float)",
                        "",
                        "# Initial guess can be omitted, and it defaults to zero vector",
                        "solution = gauss_seidel(A, b)",
                        "",
                        "print(\"Solution:\", solution)"
                      ]
                    },
                    "Analysis of Covariance, Eigenvalues, and Linear Regression": {
                      "prefix": "cov-eigen-lin-reg",
                      "body": [
                          "import numpy as np",
                          "import pandas as pd",
                          "from sklearn.linear_model import LinearRegression",
                          "from sklearn.metrics import r2_score",
                          "from sklearn.model_selection import train_test_split",
                          "",
                          "np.random.seed(42)",
                          "",
                          "n_samples = 100",
                          "n_features = 4",
                          "X = np.random.rand(n_samples, n_features) * 10",
                          "",
                          "data = pd.DataFrame(X, columns=[f\"Feature_{i+1}\" for i in range(n_features)])",
                          "",
                          "print(\"Dataset:\")",
                          "print(data.head())",
                          "",
                          "cov_matrix = np.cov(data.T)",
                          "",
                          "print(\"\\nCovariance Matrix:\")",
                          "print(cov_matrix)",
                          "",
                          "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)",
                          "",
                          "print(\"\\nEigenvalues:\")",
                          "print(eigenvalues)",
                          "",
                          "print(\"\\nEigenvectors:\")",
                          "print(eigenvectors)",
                          "",
                          "target_feature = \"Feature_1\"",
                          "predictors = [col for col in data.columns if col != target_feature]",
                          "",
                          "X_train, X_test, y_train, y_test = train_test_split(data[predictors], data[target_feature], test_size=0.2, random_state=42)",
                          "",
                          "model = LinearRegression()",
                          "model.fit(X_train, y_train)",
                          "",
                          "y_pred = model.predict(X_test)",
                          "",
                          "r2 = r2_score(y_test, y_pred)",
                          "print(f\"\\nLinear Regression R² Score (predicting {target_feature}): {r2:.4f}\")"
                      ],
                      "description": "Perform covariance analysis, eigenvalue computation, and linear regression prediction."
                  },
                    "Analysis of Results Markdown": {
                        "prefix": "analysis-results-markdown",
                        "body": [
                            "## Step 5: Analysis of Results",
                            "",
                            "### Covariance Matrix",
                            "The covariance matrix shows how strongly each feature varies with respect to the others:",
                            "- High covariance values between features suggest strong linear relationships.",
                            "- Low covariance values indicate weak or no relationships, which could mean features are independent.",
                            "",
                            "In our dataset, the covariance matrix can help determine if there is enough correlation between the predictors and the target feature (`Feature_1`).",
                            "",
                            "---",
                            "",
                            "### Eigenvalues and Eigenvectors",
                            "The **eigenvalues** of the covariance matrix represent the variance explained along each principal direction:",
                            "- Larger eigenvalues correspond to directions where the data has significant variance.",
                            "- Smaller eigenvalues indicate directions with very little variance.",
                            "",
                            "In this case:",
                            "- If one or more eigenvalues are very small, it means certain directions (features) contribute little variance and may not be informative for predicting the target feature.",
                            "",
                            "---",
                            "",
                            "### Linear Regression Performance",
                            "The R² score for the linear regression model when predicting `Feature_1` is:",
                            "",
                            "> **R² Score: -0.0063**",
                            "",
                            "#### Interpretation:",
                            "- An R² score close to zero (or negative) indicates that the linear regression model does not explain the variance in the target feature.",
                            "- Specifically, an R² of **-0.0063** means the model performs worse than a simple mean prediction, which suggests that the predictors (`Feature_2`, `Feature_3`, and `Feature_4`) have little to no linear relationship with `Feature_1`.",
                            "",
                            "---",
                            "",
                            "### Key Insights:",
                            "1. **Poor Model Performance**:",
                            "- The negative R² score indicates that the predictors do not provide useful information for predicting `Feature_1`.",
                            "- This could happen if the predictors are weakly correlated with the target feature or contain little variance (small eigenvalues).",
                            "",
                            "2. **Eigenvalues and Informative Directions**:",
                            "- If some eigenvalues are very small, the corresponding directions (features) do not contribute meaningful variance to the dataset.",
                            "- In this case, the linear regression model may struggle because there is no strong linear relationship to leverage.",
                            "",
                            "3. **Possible Redundancy or Noise**:",
                            "- The features may contain noise, redundancy, or nonlinear relationships that cannot be captured by linear regression.",
                            "- Feature engineering, transformations, or alternative models (e.g., polynomial regression or decision trees) might be needed to achieve better performance.",
                            "",
                            "---",
                            "",
                            "### Conclusion:",
                            "The poor performance of the linear regression model (R² = -0.0063) highlights the following:",
                            "- There is no strong linear relationship between the predictors and `Feature_1`.",
                            "- Further investigation is needed to identify whether:",
                            "   - The predictors are informative enough.",
                            "   - A nonlinear model or additional features could improve performance.",
                            "   - Data preprocessing or feature selection could remove noise and redundancy."
                        ],
                        "description": "Insert a detailed analysis of the results, including covariance matrix, eigenvalues, and linear regression performance."
                    }              
}
